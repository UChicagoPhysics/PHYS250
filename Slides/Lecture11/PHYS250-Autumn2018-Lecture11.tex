%----------------------------------------------------
% Setup Beamer
%----------------------------------------------------
\documentclass[hyperref={colorlinks=true}]{beamer}

%----------------------------------------------------
% Packages to use
%----------------------------------------------------
\input{../packages.sty}

%----------------------------------------------------
% Setup Theme
%----------------------------------------------------
\input{../theme.sty}

%----------------------------------------------------
% Table of Contents at each section transition
%----------------------------------------------------

\AtBeginSection[]
{
   \begin{frame}
       \frametitle{Outline}
       \setcounter{tocdepth}{2}
       \tableofcontents[currentsection]
   \end{frame}
}

%----------------------------------------------------
% Colors
%----------------------------------------------------
\input{../mycolors.sty}

%----------------------------------------------------
% Style, formatting, and new commands
%----------------------------------------------------
\input{../../global.sty}
\input{../newcommands.sty}
\input{../EandMcommands.sty}

%----------------------------------------------------
% Set paths for plots and images
%----------------------------------------------------
\input{../paths.sty}

%----------------------------------------------------
% SETTINGS FOR THIS LECTURE
%----------------------------------------------------
\newcommand{\lecnum }  {Lecture 11}
\newcommand{\lecdate}  {November 8, 2018}
\newcommand{\topic}    {ODEs, PDEs, and Fourier Transforms!}

%-----------------------------------------------------------------------------------------
% Title: [Column]{Title}
%-----------------------------------------------------------------------------------------
\title[PHYS 250 (Autumn 2018) -- \lecnum]{\topic}

%-----------------------------------------------------------------------------------------
% SubTitle: [Column]{Subtitle}
%-----------------------------------------------------------------------------------------
\subtitle{PHYS 250 (Autumn 2018) -- \lecnum}

%-----------------------------------------------------------------------------------------
% Author: [SubAuthor]{Author}
%-----------------------------------------------------------------------------------------
\author[D.W.~Miller]{David Miller}

%----------------------------------------------------
% Institute: [SubInst]{Institute}
%----------------------------------------------------
\institute[EFI, Chicago] 
{
  Department of Physics and the Enrico Fermi Institute\\
  University of Chicago
}

%----------------------------------------------------
% Institute: [SubInst]{Institute}
%----------------------------------------------------
\date[\lecdate]{\lecdate}

\subject{PHYS 250 Lecture}

\begin{document}

%==========================================================================================
% TITLE PAGE
%==========================================================================================

{
\begin{frame}
  \titlepage
\end{frame}
}

%==========================================================================================
\section[Reminders]{Reminders}
%==========================================================================================

%-----------------------------------------------------------------------------------------
\subsection[Reminders from Lecture 10]{Reminders from Lecture 10}
%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Reminders from last time}

  We discussed the concepts that grew out of (or built upon) root finding -- manipulation of Taylor series approximations -- in order to expand our computational toolkit
  
  \vspace{0.3cm}
  
  \begin{ucblock}{Application of root finding and numerical differentiation}
    \begin{itemize}
      \item \bluebf{Newton's method:} 
      \begin{itemize}
        \item Pathologies associated with the naive implementation, as well as simple modifications that can mitigate issues (\alertbf{backtracking})
        \item Multidimensional applications in matrix form and systems of equations
      \end{itemize}
      \item \bluebf{Ordinary differential equations:} 
      \begin{itemize}
        \item Obtained and discussed the Runge-Kutta algorithm(s)
        \item Differentiated between \bluebf{initial} and \alertbf{boundary} value problems   
      \end{itemize}
    \end{itemize}
  \end{ucblock}
  
  \mysp
  
  Today we will take the next steps with ODEs and transition to PDEs!

\end{frame}


%==========================================================================================
\section[Hands on with the Runge-Kutta family of methods]{Hands on with the Runge-Kutta family of methods}
%==========================================================================================

%-----------------------------------------------------------------------------------------
\subsection[Reminders]{Reminders}
%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Runge-Kutta family of algorithms}

  The aim of Runge-Kutta methods is to eliminate the need for repeated differentiation of the differential equations. Because no such differentiation is involved in the \alertbf{first-order Taylor series} expression:
  %
  \begin{equation}
    \vec{y}(x + h) = \vec{y}(x) + \vec{y}^{\prime}(x)h = \vec{y}(x) + \vec{F}(x, \vec{y})h
  \end{equation}
  
  This first-order version is referred to as \bluebf{Euler's method (aka Euler's Rule)}.
  
  \mysp
  
  Effectively, what this is doing is to start with the known initial value of the dependent variable, $y_0 \equiv y(x = 0)$, and then use the derivative function $f(x,y)$ to find an approximate value for $y$ at a small step $x = h$ forward in time; that is, $y(x = h) \equiv y_1$. 
  
  \mysp
  
  We know from our discussion of differentiation that the error in the forward-difference algorithm is $\mathcal{O}(h)$, and so this too is the error in Euler's rule.
  
    \centering \Large \alertbf{How can we test this?}

\end{frame}

%-----------------------------------------------------------------------------------------
\subsection[Code up the algorithm]{Code up the algorithm}
%-----------------------------------------------------------------------------------------

\begin{frame}[fragile]
  \frametitle{Code up the algorithm!}

  \begin{ucpythonblock}{ExplicitEuler}
    for i, x_i in enumerate(x[:-1]): 

        h = x[i+1] - x_i                        
        y[i+1] = y[i] + h*func(x_i, y[i], args) 

    return y    
  \end{ucpythonblock}
  
  \pause
  
  \begin{figure}
    \includegraphics[width=0.5\textwidth]{ExplictEulerVsAnalytic.png}
  \end{figure}

\end{frame}

%-----------------------------------------------------------------------------------------
\subsection[Test the accuracy of the methods]{Test the accuracy of the methods}
%-----------------------------------------------------------------------------------------

\begin{frame}[fragile,shrink=15]
  \frametitle{Improve the accuracy}

  We know that the accuracy depends on $h$ so make that smaller?

  \begin{ucpythonblock}{ExplicitEuler}
for N in [5, 10, 20, 40]:
    x = np.linspace(0., x_max, N+1) # Time steps
    y = ExplicitEuler(exp, y_0, x, solve_args)
    plt.plot(x, y, '-o', label='%d steps'%N) 
    
plt.plot(x,np.exp(solve_args['a']*x),'k--',label='Known')
plt.xlabel(r'$x$'), plt.ylabel(r'$y$')
plt.legend(loc=2)   
  \end{ucpythonblock}
  
  \pause
  
  \begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{ExplictEulerSteps.png}
  \end{figure}

\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}
  \frametitle{Error of the method}

  \begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{ExplictEulerAccuracyVsh.png}
  \end{figure}

\end{frame}


%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Second order Runge-Kutta family of algorithms} 

  Recall that the key insight was to expand $f(x,y)$ in a Taylor series about the \bluebf{midpoint of the integration interval} and retain two
terms:
  %
  \begin{equation}
    f(x,y) \simeq f(x_{n+1/2} , y_{n+1/2}) + (x - x_{n+1/2}) \frac{df}{dx}(x_{n+1/2}) + \mathcal{O}(h^2)
  \end{equation}
  %
  As you recall from the \bluebf{finite central difference} algorithm, only \bluebf{odd powers} of $h$ remain, and thus when used inside the integral above, the terms with $(x - x_{n+1/2})^{n\in\mathrm{odd}}$ vanish. We are left with
  %
  \begin{equation}
    y_{n+1} \simeq y_{n} + h f(x_{n+1/2} , y_{n+1/2}) + \mathcal{O}(h^2) 
  \end{equation}
  
  \centering \Large \alertbf{How can we test this?}

\end{frame}

%==========================================================================================
\section[Partial Differential Equations]{Partial Differential Equations}
%==========================================================================================

%-----------------------------------------------------------------------------------------
\subsection[Statement of the problem]{Statement of the problem}
%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{What's so different about PDEs?}

  Physical quantities such as temperature, pressure, gravitational and electromagnetic forces, and more, may vary continuously in both space and time. 
  
  \mysp
  
  As such, the functions or \bluebf{fields, $U(x,y,z,t)$} that describe these quantities must contain \alertbf{independent space and time variations}.
  
  \mysp
  
  As time evolves, the changes in $U (x, y, z, t)$ at any one position affect the field at neighboring points. 
  
  \mysp
  
  This means that the dynamic equations describing the dependence of $U$ on four independent variables must be written in terms of partial derivatives, and therefore the equations must be \alertbf{partial differential equations (PDEs)}, in contrast to ordinary differential equations (ODEs).
  
\end{frame}

%-----------------------------------------------------------------------------------------
\subsection[Classes and boundary conditions for PDEs]{Classes and boundary conditions for PDEs}
%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{General form of PDEs}

  The most general form for a two-independent variable PDE is
  \begin{figure}
    \includegraphics[width=0.8\textwidth]{GeneralPDE.pdf}
  \end{figure}
  where $A, B, C,$ and $F$ are arbitrary functions of the variables $x$ and $y$. 
 
  \pause
  
  These typically fall into three categories based on the structure of the derivatives in the expression:
  
  \begin{figure}
    \includegraphics[width=0.9\textwidth]{PDEClasses.pdf}
  \end{figure}
  


\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Boundary conditions for PDE classes}

  Separately, the boundary conditions will place specific constraints on the relationships between each of the terms, and often dictate whether the problem is even solvable.
  
  \begin{figure}
    \includegraphics[width=0.8\textwidth]{BoundaryConditions.pdf}
  \end{figure}
  
  \begin{itemize}
    \item \bluebf{Dirichlet:} value of $U$ on a surface
    \item \bluebf{Neumann:} value of the normal derivative of $U$ on a surface
    \item \bluebf{Cauchy:} value of both $U$ and $U^{\prime}$ on a surface 
  \end{itemize}


\end{frame}



%-----------------------------------------------------------------------------------------
\subsection[Example: electrostatic potentials]{Example: electrostatic potentials}
%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Example from your courses: Laplace in E\&M}

  The Laplace equation is of fundamental importance in physics, and is most often first encountered in electrodynamics.

  \begin{equation}
    \nabla^{2} V = 0.
  \end{equation}

  In E\&M, we often taught the solution to Laplace's equation in two dimensions, $V(x,y)$, which can be found by evaluating:

  \begin{equation}
    V(x,y) = \frac{1}{2\pi R} \oint_{\rm circle} V dl
  \end{equation}
  
  due to the lack of sources. In fact, even in common textbooks (e.g. Griffiths) you are taught that you can solve this iteratively, by evaluating this integral until a change in $V(x,y)$ on successive evaluations of the expression is smaller than some tolerance. 
  
\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Method of relaxation}

  To solve our 2D PDE numerically, we divide space up into a lattice (recall: \texttt{meshgrid}!) and solve for $V$ at each site on the lattice. Since we will express derivatives in terms of the finite differences in the values of $V$ at the lattice sites, this is called....\pause \bluebf{a finite-difference method}!
  
  \pause
  
  \mysp
  
  To derive the finite-difference algorithm for the numeric solution of this PDE, we would follow the same path taken before, and add the two Taylor expansions of the potential to the right and left of $(x, y)$ and above and below $(x, y)$ and so on.  
  
  \mysp
  
  At this point, I think it's just worth ``writing'' some code...
  
\end{frame}

%==========================================================================================
%==========================================================================================
\end{document}
